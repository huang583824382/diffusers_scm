import torch
# from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import StableDiffusionXLPipeline
from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl_scm import StableDiffusionXLPipeline
from torchvision.utils import save_image
from diffusers.models.unets.unet_2d_condition_scm import UNet2DConditionModel
from diffusers import AutoencoderKL, EulerDiscreteScheduler, LCMScheduler

model_path = "/mnt/afs_james/zhiwei/latent-consistency-model/stabilityai/stable-diffusion-xl-base-1.0"

trained_scm_model_path = "/mnt/afs_james/zhiwei/diffusers_scm/examples/consistency_distillation/CC12M-scm-sdxl-wds/checkpoint-1500"

generator = torch.manual_seed(0)

unet = UNet2DConditionModel.from_pretrained(
    trained_scm_model_path, subfolder="unet", torch_dtype=torch.float16
)

pipe = StableDiffusionXLPipeline.from_pretrained(
    model_path, torch_dtype=torch.float16,
    unet=unet,
    scheduler=LCMScheduler.from_pretrained(model_path, subfolder="scheduler"),
)

pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse in a futuristic city, digital art"
image = pipe(prompt, generator=generator).images[0]
image.save("astronaut_rides_horse_city_LCMScheduler.png")

"""
raw: cond 和 uncond 的mean从0.001慢慢升到0.02，根据推理步数

scm: 
从第四步开始，两者的mean不再是同一水准了
noise_pred_uncond tensor(0.0585, device='cuda:0', dtype=torch.float16)
noise_pred_text tensor(-0.0063, device='cuda:0', dtype=torch.float16)


"""

"""
scm:
emb[0] tensor([ 0.0316,  0.0112,  0.0083,  ..., -0.0528, -2.5527, -0.3174],
       device='cuda:0', dtype=torch.float16)
emb[0] tensor([ 0.0316,  0.0112,  0.0083,  ..., -0.0528, -2.5527, -0.3174],
       device='cuda:0', dtype=torch.float16)
tensor([[[[-0.4070, -0.2483,  0.9521,  ..., -0.3198, -0.4954,  0.5356],
          [ 0.3862, -1.0273,  0.4968,  ..., -0.9360,  1.6816, -1.6064],
          [-0.4067,  0.6323,  0.3354,  ...,  1.9023, -0.3591,  0.7397],
          ...,
          [-0.2004,  0.2356, -0.5332,  ...,  0.0422,  0.1536,  0.2059],
          [ 0.8579,  0.2703,  0.1819,  ..., -0.1236,  0.3547, -0.5693],
          [ 0.1864, -0.2981, -0.5024,  ..., -0.2294,  0.5317, -0.1809]],

         [[-1.4297, -1.5098, -0.7085,  ...,  0.5322,  0.0536, -0.6104],
          [ 0.9683, -0.6377,  0.3113,  ...,  0.7739, -0.8911, -1.1016],
          [-0.8213, -0.9282,  0.1296,  ...,  0.2301,  0.1164, -1.0498],
          ...,
          [ 0.1859,  0.0117,  0.5161,  ...,  0.2465,  0.1815, -0.2026],
          [ 0.9473,  1.1123, -0.0276,  ...,  0.5767,  0.4724,  0.0928],
          [-0.8115,  0.6147,  0.1871,  ..., -0.7300,  0.2725, -0.2141]],

         [[ 0.4736,  0.4346,  0.5400,  ..., -0.2012,  1.6807, -0.0956],
          [-0.5337,  0.0597,  0.5093,  ..., -1.2529, -0.3176, -1.7031],
          [ 1.5996, -1.1611,  0.7627,  ...,  0.1598, -0.8882, -0.6572],
          ...,
          [-0.0521,  0.3462,  0.3154,  ..., -0.1001, -0.2390, -0.2959],
          [-0.4490, -0.6392,  0.1096,  ...,  0.3342, -0.3572,  1.2871],
          [-0.9790, -0.5493, -0.0072,  ..., -0.1064, -0.1903, -0.2411]],

         [[ 0.4260, -1.0420, -0.7607,  ..., -0.2155, -0.5562, -1.1875],
          [-2.5625,  1.5928,  1.1143,  ..., -1.1133,  1.3242,  2.5664],
          [ 1.2334, -0.3308, -0.1089,  ...,  0.0059,  1.5449, -0.7715],
          ...,
          [ 0.0645, -0.1704,  0.2450,  ...,  0.1713,  0.2372,  0.8330],
          [-0.0510,  0.1636, -0.2400,  ...,  0.2137,  0.1414, -0.2986],
          [ 0.1156, -1.2295, -0.1871,  ..., -1.1846,  0.7700, -0.7383]]]],
       device='cuda:0', dtype=torch.float16)
tensor([[[[-0.4236, -0.2559,  0.9521,  ..., -0.3235, -0.5107,  0.5239],
          [ 0.3865, -1.0381,  0.4849,  ..., -0.9312,  1.6709, -1.5928],
          [-0.4304,  0.6162,  0.3477,  ...,  1.9102, -0.3616,  0.7183],
          ...,
          [-0.1971,  0.2386, -0.5391,  ...,  0.0612,  0.1390,  0.1992],
          [ 0.8677,  0.2693,  0.1903,  ..., -0.1254,  0.3562, -0.5542],
          [ 0.2017, -0.3098, -0.4993,  ..., -0.2311,  0.5156, -0.1820]],

         [[-1.4170, -1.5225, -0.7202,  ...,  0.5259,  0.0784, -0.5938],
          [ 0.9556, -0.6567,  0.3181,  ...,  0.7891, -0.8970, -1.0869],
          [-0.8398, -0.9385,  0.1315,  ...,  0.2009,  0.1163, -1.1074],
          ...,
          [ 0.2031,  0.0179,  0.5059,  ...,  0.2498,  0.1677, -0.1821],
          [ 0.9551,  1.1006, -0.0249,  ...,  0.5771,  0.4612,  0.0860],
          [-0.8037,  0.6260,  0.1781,  ..., -0.7275,  0.2710, -0.2269]],

         [[ 0.4983,  0.4316,  0.5503,  ..., -0.1981,  1.6572, -0.1169],
          [-0.5312,  0.0573,  0.5127,  ..., -1.2578, -0.2859, -1.6699],
          [ 1.5938, -1.1709,  0.7666,  ...,  0.1193, -0.8804, -0.6621],
          ...,
          [-0.0468,  0.3301,  0.3171,  ..., -0.0998, -0.2490, -0.2930],
          [-0.4541, -0.6523,  0.1088,  ...,  0.3245, -0.3489,  1.2930],
          [-0.9727, -0.5698, -0.0083,  ..., -0.1089, -0.1941, -0.2515]],

         [[ 0.4631, -1.0605, -0.7837,  ..., -0.2095, -0.5439, -1.1738],
          [-2.5645,  1.6016,  1.1074,  ..., -1.1074,  1.2822,  2.4824],
          [ 1.2451, -0.3440, -0.1034,  ..., -0.0099,  1.6064, -0.7729],
          ...,
          [ 0.0572, -0.1718,  0.2402,  ...,  0.1780,  0.2266,  0.8071],
          [-0.0599,  0.1670, -0.2520,  ...,  0.2124,  0.1389, -0.2739],
          [ 0.1178, -1.2334, -0.1847,  ..., -1.1924,  0.7754, -0.7368]]]],
       device='cuda:0', dtype=torch.float16)

raw:
emb[0] tensor([ 0.0316,  0.0112,  0.0083,  ..., -0.0528, -2.5527, -0.3174],
       device='cuda:0', dtype=torch.float16)
emb[0] tensor([ 0.0316,  0.0112,  0.0083,  ..., -0.0528, -2.5527, -0.3174],
       device='cuda:0', dtype=torch.float16)
tensor([[[[ 1.5186,  1.1494, -0.1713,  ...,  0.4509, -0.6118, -1.4404],
          [ 0.2913,  0.7710, -0.1558,  ...,  0.7646, -0.0416,  1.2275],
          [-1.3486, -1.9072,  0.4622,  ..., -0.3984,  0.4895,  0.3840],
          ...,
          [-0.1803,  0.2732, -0.7720,  ...,  0.0156, -1.2246, -1.8398],
          [-0.5928,  0.3887,  0.1089,  ...,  0.3787,  0.4006,  0.6255],
          [ 0.1849, -0.2966, -1.1387,  ...,  0.1071, -0.0296, -0.9590]],

         [[-0.6841,  0.6572, -0.3323,  ..., -0.9478, -1.4492,  0.7466],
          [ 0.6768,  1.3584,  0.3330,  ...,  2.3164, -1.4521, -0.8384],
          [-0.9302,  0.7061, -0.2129,  ..., -1.1895,  0.4343,  0.2556],
          ...,
          [-0.7954, -0.2349, -0.9766,  ..., -0.1599,  0.3179, -0.9082],
          [ 0.7222,  0.0284,  1.2012,  ...,  0.9141, -0.1859, -0.1766],
          [ 0.4099, -0.7202,  0.3818,  ...,  0.5132,  0.8726, -0.0614]],

         [[ 0.1018, -0.7192,  1.1729,  ..., -0.6851, -0.0146,  1.9092],
          [-1.5781,  0.1151,  0.2358,  ...,  1.4033, -0.6562,  0.0851],
          [-0.0453, -1.2764,  1.3799,  ...,  1.8281, -0.4773, -0.2294],
          ...,
          [ 0.5176,  0.4502,  0.7959,  ..., -0.1913,  1.0664,  0.6885],
          [-0.2039, -0.5635,  1.3564,  ..., -0.0693,  0.2483, -0.6211],
          [-0.5391, -0.3923, -0.3901,  ...,  0.3567,  0.5723,  0.7974]],

         [[ 1.5273, -0.7085, -0.1565,  ..., -1.7861,  0.0875,  0.2698],
          [-0.8477, -0.9243, -1.1250,  ...,  0.2092,  0.3638, -0.6123],
          [ 0.8696,  0.2583, -0.3279,  ..., -0.2573,  0.0729, -0.5371],
          ...,
          [ 0.5156,  0.6528,  0.1241,  ..., -0.7134,  0.9814, -0.4053],
          [ 0.9688, -0.0339,  0.4365,  ...,  0.9956,  0.3862, -0.4749],
          [-0.6519, -0.1143, -0.1329,  ...,  0.3491,  0.6982, -0.4797]]]],
       device='cuda:0', dtype=torch.float16)
tensor([[[[ 1.5234,  1.1611, -0.1615,  ...,  0.4780, -0.6045, -1.4326],
          [ 0.2720,  0.7773, -0.1609,  ...,  0.7808, -0.0093,  1.2441],
          [-1.3447, -1.9180,  0.4746,  ..., -0.3730,  0.4927,  0.3962],
          ...,
          [-0.1807,  0.2603, -0.7632,  ...,  0.0222, -1.2188, -1.8584],
          [-0.5781,  0.3787,  0.1013,  ...,  0.3694,  0.3950,  0.6328],
          [ 0.1899, -0.3169, -1.1289,  ...,  0.0994, -0.0282, -0.9834]],

         [[-0.6641,  0.6426, -0.3416,  ..., -0.9463, -1.4502,  0.7593],
          [ 0.6602,  1.3652,  0.3340,  ...,  2.3203, -1.4199, -0.8281],
          [-0.9438,  0.7129, -0.2057,  ..., -1.1914,  0.4282,  0.2629],
          ...,
          [-0.7983, -0.2295, -0.9580,  ..., -0.1693,  0.3254, -0.9292],
          [ 0.7344,  0.0231,  1.1982,  ...,  0.8892, -0.1936, -0.1766],
          [ 0.4185, -0.6963,  0.3989,  ...,  0.5171,  0.8687, -0.0714]],

         [[ 0.0999, -0.7202,  1.1689,  ..., -0.6963,  0.0033,  1.9102],
          [-1.5342,  0.1281,  0.2429,  ...,  1.3955, -0.6704,  0.0894],
          [-0.0479, -1.2764,  1.3779,  ...,  1.8242, -0.4634, -0.2189],
          ...,
          [ 0.5161,  0.4565,  0.7896,  ..., -0.1907,  1.0781,  0.7139],
          [-0.2274, -0.5586,  1.3545,  ..., -0.0858,  0.2642, -0.6157],
          [-0.5391, -0.3821, -0.3875,  ...,  0.3438,  0.5625,  0.8379]],

         [[ 1.5371, -0.7051, -0.1423,  ..., -1.7871,  0.0560,  0.2734],
          [-0.9004, -0.9229, -1.1221,  ...,  0.1995,  0.3621, -0.6250],
          [ 0.8887,  0.2771, -0.3135,  ..., -0.2505,  0.0673, -0.5439],
          ...,
          [ 0.5142,  0.6504,  0.1355,  ..., -0.7124,  0.9502, -0.4165],
          [ 0.9941, -0.0195,  0.4224,  ...,  0.9980,  0.3831, -0.4697],
          [-0.6470, -0.1180, -0.1216,  ...,  0.3599,  0.6895, -0.4851]]]],
       device='cuda:0', dtype=torch.float16)


"""

"""
scm:
in resnet block temb: tensor([-1.1346e-01, -6.5576e-01,  9.2920e-01, -4.6948e-01, -2.6831e-01,
         2.3877e-01, -2.2400e-01, -1.2659e-01,  3.3472e-01,  1.6431e-01,
        -2.6587e-01,  2.2119e-01, -4.1187e-01,  7.5317e-02, -1.1792e-01,
        -3.2495e-01,  1.3848e+00, -2.0430e+00,  6.6064e-01,  1.4248e+00,
         5.1331e-02,  2.2986e-01,  1.4160e-01,  3.3398e-01,  1.6235e-01,
         6.4990e-01,  9.6143e-01,  4.8218e-01, -1.3513e-01, -8.4668e-01,
        -3.5675e-02,  3.5305e-03, -4.3896e-01, -1.7834e-01, -1.0486e-01,
         1.0596e+00,  2.3413e-01,  3.4131e-01, -1.0098e+00,  1.2164e-01,
         8.1116e-02, -3.0615e-01,  8.6609e-02,  4.4360e-01, -8.7219e-02,
         1.6882e-01, -4.0381e-01,  7.0923e-02,  1.8347e-01, -1.0315e-01,
         3.5596e-01, -9.0332e-01,  4.2432e-01, -8.2947e-02, -3.2617e-01,
         5.7031e-01,  6.1768e-01, -2.6138e-02, -5.3125e-01,  3.8525e-01,
         8.9307e-01, -1.3110e-01,  4.0039e-01,  3.6792e-01,  1.3940e-01,
         8.1348e-01,  6.5735e-02, -3.7109e-02,  2.8564e-01, -4.3018e-01,
        -3.9136e-01,  6.3330e-01,  1.1871e-01, -8.0633e-04,  3.2617e-01,
        -1.5781e+00,  4.5923e-01,  3.1519e-01,  7.8857e-01,  1.4287e+00,
         1.9678e-01,  1.0077e-01,  8.8574e-01,  3.0444e-01,  1.3306e-01,
         6.2073e-02, -7.5195e-02,  4.2023e-02, -9.3311e-01, -5.0507e-02,
         1.4087e-01, -2.6172e-01,  4.3164e-01, -5.0488e-01, -3.0859e-01,
        -7.6721e-02, -3.7427e-01, -3.2397e-01,  1.5942e-01,  1.5198e-01,
         7.8174e-01,  6.1859e-02,  1.4905e-01,  7.2388e-02, -9.7900e-02,
        -2.5415e-01,  2.2171e-02,  1.6931e-01, -7.1191e-01, -1.5540e-01,
         2.0557e-01, -1.5588e-01, -2.2070e-01,  8.9111e-02, -7.8125e-02,
         3.3350e-01,  1.0785e-01,  2.3669e-01,  1.8570e-02,  1.0605e+00,
         2.2058e-01,  1.1774e-01,  5.4395e-01, -5.1611e-01, -3.2202e-01,
         3.0835e-01,  6.8420e-02,  2.1301e-01, -2.1619e-01,  3.6621e-01,
        -1.4062e-01,  4.5508e-01,  2.1167e-01,  6.4551e-01, -5.8289e-02,
         1.4722e-01, -4.1650e-01, -1.2490e+00,  4.9878e-01, -1.5967e-01,
        -3.4271e-02, -3.8940e-01, -2.1460e-01,  7.4768e-02,  1.1243e-01,
         1.8274e-01, -5.3925e-02, -6.3354e-02, -3.3203e-01,  9.6863e-02,
        -1.8652e-01, -1.3745e-01, -4.9316e-01,  4.0771e-01, -3.4888e-01,
        -3.5791e-01,  4.1901e-02, -3.6743e-01,  1.2734e+00,  6.8054e-02,
        -1.7969e-01,  8.8086e-01,  2.2424e-01,  2.3694e-01,  2.6904e-01,
         1.2524e-01, -1.5356e-01, -8.4277e-01, -2.3413e-01,  7.6233e-02,
        -3.9819e-01,  2.0332e-03, -6.4697e-01,  1.6104e+00, -4.5801e-01,
         1.3408e+00,  1.8103e-01, -3.3447e-01, -1.0459e+00, -1.6338e+00,
         4.2065e-01,  8.4863e-01,  2.3361e-02,  4.8315e-01, -7.6904e-01,
        -3.6279e-01,  4.6802e-01,  4.5990e-02,  1.3330e-01, -6.2109e-01,
         3.8110e-01, -1.6006e+00,  2.2986e-01,  8.1543e-01, -3.3496e-01,
        -3.9276e-02, -1.9043e-01,  3.0200e-01,  4.4067e-01,  3.7720e-01,
        -4.7656e-01, -4.0802e-02, -5.6543e-01, -1.2219e-01, -2.8076e-01,
         5.8203e-01, -1.5596e+00,  3.3447e-01, -2.0654e-01,  6.9336e-01,
        -3.2056e-01,  1.5967e-01,  1.5190e-02,  1.0723e+00,  2.5708e-01,
         6.5979e-02,  2.0874e-01, -2.5562e-01,  1.5784e-01,  1.2708e-01,
         7.2266e-01, -6.3721e-01, -1.4612e-01, -9.8877e-02,  3.4717e-01,
        -5.5145e-02, -4.3799e-01,  9.0599e-04, -6.7017e-02, -4.1284e-01,
         2.6465e-01, -3.1079e-01,  1.4131e+00,  1.2783e+00, -2.4658e-01,
         1.5015e-01, -7.0068e-02, -5.1660e-01, -3.8574e-01, -2.1973e+00,
         1.2927e-01, -7.0923e-02, -1.1053e-01,  3.8745e-01, -2.5024e-01,
        -5.3906e-01, -3.0054e-01, -2.1704e-01,  2.3779e-01,  3.3478e-02,
         8.6670e-03, -1.1360e-02,  2.1826e-01, -5.6122e-02,  8.3691e-01,
        -8.4570e-01,  3.5669e-01, -1.6394e-01, -2.5903e-01,  1.3867e-01,
        -1.9312e-01,  1.9226e-01, -2.8076e-01, -2.2559e-01,  5.5615e-01,
        -7.8674e-02,  1.4807e-01, -4.0430e-01, -2.7051e-01, -8.8232e-01,
         3.1152e-01, -1.7285e-01, -1.8201e-01,  8.4229e-02,  3.6719e-01,
        -6.0616e-03, -2.1216e-01, -1.7737e-01, -4.1528e-01, -1.6650e-01,
         3.8794e-01,  2.5928e-01, -1.0449e-01,  2.5854e-01, -7.1289e-02,
         6.7578e-01,  4.6606e-01, -2.3840e-01, -3.8013e-01,  4.4897e-01,
        -5.9570e-01,  3.5522e-01,  1.3330e+00,  1.9446e-01,  1.9434e-01,
        -4.8340e-01, -1.2734e+00, -6.2109e-01,  7.6709e-01, -7.7393e-01,
        -4.2407e-01,  4.9023e-01, -2.1692e-01,  5.9521e-01, -1.2830e-01,
        -1.9006e-01, -8.7646e-01, -3.2153e-01,  9.8584e-01,  6.5491e-02,
        -6.6064e-01,  4.6558e-01, -1.5039e-01,  6.0986e-01,  6.5430e-01,
         6.8164e-01, -4.1821e-01, -3.1201e-01,  2.3096e-01,  1.9067e-01],
       device='cuda:0', dtype=torch.float16)
temb.shape: torch.Size([2, 320, 128, 128])


raw:
in resnet block temb: tensor([-1.1346e-01, -6.5576e-01,  9.2920e-01, -4.6948e-01, -2.6831e-01,
         2.3877e-01, -2.2400e-01, -1.2659e-01,  3.3472e-01,  1.6431e-01,
        -2.6587e-01,  2.2119e-01, -4.1187e-01,  7.5317e-02, -1.1792e-01,
        -3.2495e-01,  1.3848e+00, -2.0430e+00,  6.6064e-01,  1.4248e+00,
         5.1331e-02,  2.2986e-01,  1.4160e-01,  3.3398e-01,  1.6235e-01,
         6.4990e-01,  9.6143e-01,  4.8218e-01, -1.3513e-01, -8.4668e-01,
        -3.5675e-02,  3.5305e-03, -4.3896e-01, -1.7834e-01, -1.0486e-01,
         1.0596e+00,  2.3413e-01,  3.4131e-01, -1.0098e+00,  1.2164e-01,
         8.1116e-02, -3.0615e-01,  8.6609e-02,  4.4360e-01, -8.7219e-02,
         1.6882e-01, -4.0381e-01,  7.0923e-02,  1.8347e-01, -1.0315e-01,
         3.5596e-01, -9.0332e-01,  4.2432e-01, -8.2947e-02, -3.2617e-01,
         5.7031e-01,  6.1768e-01, -2.6138e-02, -5.3125e-01,  3.8525e-01,
         8.9307e-01, -1.3110e-01,  4.0039e-01,  3.6792e-01,  1.3940e-01,
         8.1348e-01,  6.5735e-02, -3.7109e-02,  2.8564e-01, -4.3018e-01,
        -3.9136e-01,  6.3330e-01,  1.1871e-01, -8.0633e-04,  3.2617e-01,
        -1.5781e+00,  4.5923e-01,  3.1519e-01,  7.8857e-01,  1.4287e+00,
         1.9678e-01,  1.0077e-01,  8.8574e-01,  3.0444e-01,  1.3306e-01,
         6.2073e-02, -7.5195e-02,  4.2023e-02, -9.3311e-01, -5.0507e-02,
         1.4087e-01, -2.6172e-01,  4.3164e-01, -5.0488e-01, -3.0859e-01,
        -7.6721e-02, -3.7427e-01, -3.2397e-01,  1.5942e-01,  1.5198e-01,
         7.8174e-01,  6.1859e-02,  1.4905e-01,  7.2388e-02, -9.7900e-02,
        -2.5415e-01,  2.2171e-02,  1.6931e-01, -7.1191e-01, -1.5540e-01,
         2.0557e-01, -1.5588e-01, -2.2070e-01,  8.9111e-02, -7.8125e-02,
         3.3350e-01,  1.0785e-01,  2.3669e-01,  1.8570e-02,  1.0605e+00,
         2.2058e-01,  1.1774e-01,  5.4395e-01, -5.1611e-01, -3.2202e-01,
         3.0835e-01,  6.8420e-02,  2.1301e-01, -2.1619e-01,  3.6621e-01,
        -1.4062e-01,  4.5508e-01,  2.1167e-01,  6.4551e-01, -5.8289e-02,
         1.4722e-01, -4.1650e-01, -1.2490e+00,  4.9878e-01, -1.5967e-01,
        -3.4271e-02, -3.8940e-01, -2.1460e-01,  7.4768e-02,  1.1243e-01,
         1.8274e-01, -5.3925e-02, -6.3354e-02, -3.3203e-01,  9.6863e-02,
        -1.8652e-01, -1.3745e-01, -4.9316e-01,  4.0771e-01, -3.4888e-01,
        -3.5791e-01,  4.1901e-02, -3.6743e-01,  1.2734e+00,  6.8054e-02,
        -1.7969e-01,  8.8086e-01,  2.2424e-01,  2.3694e-01,  2.6904e-01,
         1.2524e-01, -1.5356e-01, -8.4277e-01, -2.3413e-01,  7.6233e-02,
        -3.9819e-01,  2.0332e-03, -6.4697e-01,  1.6104e+00, -4.5801e-01,
         1.3408e+00,  1.8103e-01, -3.3447e-01, -1.0459e+00, -1.6338e+00,
         4.2065e-01,  8.4863e-01,  2.3361e-02,  4.8315e-01, -7.6904e-01,
        -3.6279e-01,  4.6802e-01,  4.5990e-02,  1.3330e-01, -6.2109e-01,
         3.8110e-01, -1.6006e+00,  2.2986e-01,  8.1543e-01, -3.3496e-01,
        -3.9276e-02, -1.9043e-01,  3.0200e-01,  4.4067e-01,  3.7720e-01,
        -4.7656e-01, -4.0802e-02, -5.6543e-01, -1.2219e-01, -2.8076e-01,
         5.8203e-01, -1.5596e+00,  3.3447e-01, -2.0654e-01,  6.9336e-01,
        -3.2056e-01,  1.5967e-01,  1.5190e-02,  1.0723e+00,  2.5708e-01,
         6.5979e-02,  2.0874e-01, -2.5562e-01,  1.5784e-01,  1.2708e-01,
         7.2266e-01, -6.3721e-01, -1.4612e-01, -9.8877e-02,  3.4717e-01,
        -5.5145e-02, -4.3799e-01,  9.0599e-04, -6.7017e-02, -4.1284e-01,
         2.6465e-01, -3.1079e-01,  1.4131e+00,  1.2783e+00, -2.4658e-01,
         1.5015e-01, -7.0068e-02, -5.1660e-01, -3.8574e-01, -2.1973e+00,
         1.2927e-01, -7.0923e-02, -1.1053e-01,  3.8745e-01, -2.5024e-01,
        -5.3906e-01, -3.0054e-01, -2.1704e-01,  2.3779e-01,  3.3478e-02,
         8.6670e-03, -1.1360e-02,  2.1826e-01, -5.6122e-02,  8.3691e-01,
        -8.4570e-01,  3.5669e-01, -1.6394e-01, -2.5903e-01,  1.3867e-01,
        -1.9312e-01,  1.9226e-01, -2.8076e-01, -2.2559e-01,  5.5615e-01,
        -7.8674e-02,  1.4807e-01, -4.0430e-01, -2.7051e-01, -8.8232e-01,
         3.1152e-01, -1.7285e-01, -1.8201e-01,  8.4229e-02,  3.6719e-01,
        -6.0616e-03, -2.1216e-01, -1.7737e-01, -4.1528e-01, -1.6650e-01,
         3.8794e-01,  2.5928e-01, -1.0449e-01,  2.5854e-01, -7.1289e-02,
         6.7578e-01,  4.6606e-01, -2.3840e-01, -3.8013e-01,  4.4897e-01,
        -5.9570e-01,  3.5522e-01,  1.3330e+00,  1.9446e-01,  1.9434e-01,
        -4.8340e-01, -1.2734e+00, -6.2109e-01,  7.6709e-01, -7.7393e-01,
        -4.2407e-01,  4.9023e-01, -2.1692e-01,  5.9521e-01, -1.2830e-01,
        -1.9006e-01, -8.7646e-01, -3.2153e-01,  9.8584e-01,  6.5491e-02,
        -6.6064e-01,  4.6558e-01, -1.5039e-01,  6.0986e-01,  6.5430e-01,
         6.8164e-01, -4.1821e-01, -3.1201e-01,  2.3096e-01,  1.9067e-01],
       device='cuda:0', dtype=torch.float16)
temb.shape: torch.Size([2, 320, 1, 1])
"""


"""
scm:
get_time_embed tensor([5.4053e-01, 5.8643e-01, 6.2842e-01, 6.6650e-01, 7.0068e-01, 7.3193e-01,
        7.5977e-01, 7.8467e-01, 8.0762e-01, 8.2764e-01, 8.4619e-01, 8.6230e-01,
        8.7695e-01, 8.9014e-01, 9.0186e-01, 9.1260e-01, 9.2188e-01, 9.3018e-01,
        9.3750e-01, 9.4434e-01, 9.5020e-01, 9.5557e-01, 9.6045e-01, 9.6484e-01,
        9.6875e-01, 9.7217e-01, 9.7510e-01, 9.7754e-01, 9.7998e-01, 9.8242e-01,
        9.8438e-01, 9.8584e-01, 9.8730e-01, 9.8877e-01, 9.9023e-01, 9.9121e-01,
        9.9219e-01, 9.9316e-01, 9.9365e-01, 9.9463e-01, 9.9512e-01, 9.9561e-01,
        9.9609e-01, 9.9658e-01, 9.9707e-01, 9.9707e-01, 9.9756e-01, 9.9756e-01,
        9.9805e-01, 9.9805e-01, 9.9854e-01, 9.9854e-01, 9.9854e-01, 9.9902e-01,
        9.9902e-01, 9.9902e-01, 9.9902e-01, 9.9951e-01, 9.9951e-01, 9.9951e-01,
        9.9951e-01, 9.9951e-01, 9.9951e-01, 9.9951e-01, 9.9951e-01, 9.9951e-01,
        9.9951e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 8.4131e-01, 8.1006e-01,
        7.7783e-01, 7.4561e-01, 7.1338e-01, 6.8164e-01, 6.5039e-01, 6.1963e-01,
        5.8984e-01, 5.6104e-01, 5.3320e-01, 5.0635e-01, 4.8047e-01, 4.5581e-01,
        4.3188e-01, 4.0942e-01, 3.8770e-01, 3.6694e-01, 3.4741e-01, 3.2886e-01,
        3.1104e-01, 2.9419e-01, 2.7808e-01, 2.6294e-01, 2.4854e-01, 2.3486e-01,
        2.2205e-01, 2.0984e-01, 1.9824e-01, 1.8726e-01, 1.7688e-01, 1.6711e-01,
        1.5784e-01, 1.4905e-01, 1.4075e-01, 1.3293e-01, 1.2561e-01, 1.1859e-01,
        1.1194e-01, 1.0571e-01, 9.9854e-02, 9.4238e-02, 8.8989e-02, 8.4045e-02,
        7.9346e-02, 7.4890e-02, 7.0740e-02, 6.6772e-02, 6.3049e-02, 5.9540e-02,
        5.6213e-02, 5.3070e-02, 5.0110e-02, 4.7302e-02, 4.4647e-02, 4.2145e-02,
        3.9795e-02, 3.7567e-02, 3.5461e-02, 3.3478e-02, 3.1616e-02, 2.9846e-02,
        2.8183e-02, 2.6611e-02, 2.5116e-02, 2.3712e-02, 2.2385e-02, 2.1133e-02,
        1.9958e-02, 1.8829e-02, 1.7776e-02, 1.6785e-02, 1.5854e-02, 1.4961e-02,
        1.4122e-02, 1.3336e-02, 1.2589e-02, 1.1887e-02, 1.1223e-02, 1.0590e-02,
        1.0002e-02, 9.4376e-03, 8.9111e-03, 8.4152e-03, 7.9422e-03, 7.4997e-03,
        7.0801e-03, 6.6833e-03, 6.3095e-03, 5.9547e-03, 5.6229e-03, 5.3101e-03,
        5.0125e-03, 4.7302e-03, 4.4670e-03, 4.2152e-03, 3.9825e-03, 3.7575e-03,
        3.5477e-03, 3.3493e-03, 3.1624e-03, 2.9850e-03, 2.8191e-03, 2.6608e-03,
        2.5120e-03, 2.3708e-03, 2.2392e-03, 2.1133e-03, 1.9951e-03, 1.8835e-03,
        1.7786e-03, 1.6785e-03, 1.5850e-03, 1.4963e-03, 1.4124e-03, 1.3332e-03,
        1.2589e-03, 1.1883e-03, 1.1225e-03, 1.0595e-03, 1.0004e-03, 9.4414e-04,
        8.9121e-04, 8.4162e-04, 7.9441e-04, 7.5006e-04, 7.0810e-04, 6.6853e-04,
        6.3086e-04, 5.9557e-04, 5.6219e-04, 5.3072e-04, 5.0116e-04, 4.7326e-04,
        4.4680e-04, 4.2176e-04, 3.9816e-04, 3.7575e-04, 3.5477e-04, 3.3498e-04,
        3.1614e-04, 2.9850e-04, 2.8181e-04, 2.6608e-04, 2.5129e-04, 2.3711e-04,
        2.2388e-04, 2.1136e-04, 1.9956e-04, 1.8835e-04, 1.7786e-04, 1.6785e-04,
        1.5855e-04, 1.4961e-04, 1.4126e-04, 1.3340e-04, 1.2589e-04, 1.1885e-04,
        1.1218e-04, 1.0592e-04], device='cuda:0', dtype=torch.float16)
emb[0] tensor([ 0.0316,  0.0112,  0.0083,  ..., -0.0528, -2.5527, -0.3174],
       device='cuda:0', dtype=torch.float16)
       
raw:    
get_time_embed tensor([5.4053e-01, 5.8643e-01, 6.2842e-01, 6.6650e-01, 7.0068e-01, 7.3193e-01,
        7.5977e-01, 7.8467e-01, 8.0762e-01, 8.2764e-01, 8.4619e-01, 8.6230e-01,
        8.7695e-01, 8.9014e-01, 9.0186e-01, 9.1260e-01, 9.2188e-01, 9.3018e-01,
        9.3750e-01, 9.4434e-01, 9.5020e-01, 9.5557e-01, 9.6045e-01, 9.6484e-01,
        9.6875e-01, 9.7217e-01, 9.7510e-01, 9.7754e-01, 9.7998e-01, 9.8242e-01,
        9.8438e-01, 9.8584e-01, 9.8730e-01, 9.8877e-01, 9.9023e-01, 9.9121e-01,
        9.9219e-01, 9.9316e-01, 9.9365e-01, 9.9463e-01, 9.9512e-01, 9.9561e-01,
        9.9609e-01, 9.9658e-01, 9.9707e-01, 9.9707e-01, 9.9756e-01, 9.9756e-01,
        9.9805e-01, 9.9805e-01, 9.9854e-01, 9.9854e-01, 9.9854e-01, 9.9902e-01,
        9.9902e-01, 9.9902e-01, 9.9902e-01, 9.9951e-01, 9.9951e-01, 9.9951e-01,
        9.9951e-01, 9.9951e-01, 9.9951e-01, 9.9951e-01, 9.9951e-01, 9.9951e-01,
        9.9951e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 8.4131e-01, 8.1006e-01,
        7.7783e-01, 7.4561e-01, 7.1338e-01, 6.8164e-01, 6.5039e-01, 6.1963e-01,
        5.8984e-01, 5.6104e-01, 5.3320e-01, 5.0635e-01, 4.8047e-01, 4.5581e-01,
        4.3188e-01, 4.0942e-01, 3.8770e-01, 3.6694e-01, 3.4741e-01, 3.2886e-01,
        3.1104e-01, 2.9419e-01, 2.7808e-01, 2.6294e-01, 2.4854e-01, 2.3486e-01,
        2.2205e-01, 2.0984e-01, 1.9824e-01, 1.8726e-01, 1.7688e-01, 1.6711e-01,
        1.5784e-01, 1.4905e-01, 1.4075e-01, 1.3293e-01, 1.2561e-01, 1.1859e-01,
        1.1194e-01, 1.0571e-01, 9.9854e-02, 9.4238e-02, 8.8989e-02, 8.4045e-02,
        7.9346e-02, 7.4890e-02, 7.0740e-02, 6.6772e-02, 6.3049e-02, 5.9540e-02,
        5.6213e-02, 5.3070e-02, 5.0110e-02, 4.7302e-02, 4.4647e-02, 4.2145e-02,
        3.9795e-02, 3.7567e-02, 3.5461e-02, 3.3478e-02, 3.1616e-02, 2.9846e-02,
        2.8183e-02, 2.6611e-02, 2.5116e-02, 2.3712e-02, 2.2385e-02, 2.1133e-02,
        1.9958e-02, 1.8829e-02, 1.7776e-02, 1.6785e-02, 1.5854e-02, 1.4961e-02,
        1.4122e-02, 1.3336e-02, 1.2589e-02, 1.1887e-02, 1.1223e-02, 1.0590e-02,
        1.0002e-02, 9.4376e-03, 8.9111e-03, 8.4152e-03, 7.9422e-03, 7.4997e-03,
        7.0801e-03, 6.6833e-03, 6.3095e-03, 5.9547e-03, 5.6229e-03, 5.3101e-03,
        5.0125e-03, 4.7302e-03, 4.4670e-03, 4.2152e-03, 3.9825e-03, 3.7575e-03,
        3.5477e-03, 3.3493e-03, 3.1624e-03, 2.9850e-03, 2.8191e-03, 2.6608e-03,
        2.5120e-03, 2.3708e-03, 2.2392e-03, 2.1133e-03, 1.9951e-03, 1.8835e-03,
        1.7786e-03, 1.6785e-03, 1.5850e-03, 1.4963e-03, 1.4124e-03, 1.3332e-03,
        1.2589e-03, 1.1883e-03, 1.1225e-03, 1.0595e-03, 1.0004e-03, 9.4414e-04,
        8.9121e-04, 8.4162e-04, 7.9441e-04, 7.5006e-04, 7.0810e-04, 6.6853e-04,
        6.3086e-04, 5.9557e-04, 5.6219e-04, 5.3072e-04, 5.0116e-04, 4.7326e-04,
        4.4680e-04, 4.2176e-04, 3.9816e-04, 3.7575e-04, 3.5477e-04, 3.3498e-04,
        3.1614e-04, 2.9850e-04, 2.8181e-04, 2.6608e-04, 2.5129e-04, 2.3711e-04,
        2.2388e-04, 2.1136e-04, 1.9956e-04, 1.8835e-04, 1.7786e-04, 1.6785e-04,
        1.5855e-04, 1.4961e-04, 1.4126e-04, 1.3340e-04, 1.2589e-04, 1.1885e-04,
        1.1218e-04, 1.0592e-04], device='cuda:0', dtype=torch.float16)
emb[0] tensor([ 0.0316,  0.0112,  0.0083,  ..., -0.0528, -2.5527, -0.3174],
       device='cuda:0', dtype=torch.float16)
"""



"""
    UNet2DConditionModel(
  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_proj): Timesteps()
  (time_embedding): TimestepEmbedding(
    (linear_1): Linear(in_features=320, out_features=1280, bias=True)
    (act): SiLU()
    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)
  )
  (add_time_proj): Timesteps()
  (add_embedding): TimestepEmbedding(
    (linear_1): Linear(in_features=2816, out_features=1280, bias=True)
    (act): SiLU()
    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)
  )
  (down_blocks): ModuleList(
    (0): DownBlock2D(
      (resnets): ModuleList(
        (0-1): 2 x ResnetBlock2D(
          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
        )
      )
      (downsamplers): ModuleList(
        (0): Downsample2D(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
      )
    )
    (1): CrossAttnDownBlock2D(
      (attentions): ModuleList(
        (0-1): 2 x Transformer2DModel(
          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=640, out_features=640, bias=True)
          (transformer_blocks): ModuleList(
            (0-1): 2 x BasicTransformerBlock(
              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn1): Attention(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=640, out_features=640, bias=False)
                (to_v): Linear(in_features=640, out_features=640, bias=False)
                (to_out): ModuleList(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn2): Attention(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=2048, out_features=640, bias=False)
                (to_v): Linear(in_features=2048, out_features=640, bias=False)
                (to_out): ModuleList(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (ff): FeedForward(
                (net): ModuleList(
                  (0): GEGLU(
                    (proj): Linear(in_features=640, out_features=5120, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=2560, out_features=640, bias=True)
                )
              )
            )
          )
          (proj_out): Linear(in_features=640, out_features=640, bias=True)
        )
      )
      (resnets): ModuleList(
        (0): ResnetBlock2D(
          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): ResnetBlock2D(
          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
        )
      )
      (downsamplers): ModuleList(
        (0): Downsample2D(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
      )
    )
    (2): CrossAttnDownBlock2D(
      (attentions): ModuleList(
        (0-1): 2 x Transformer2DModel(
          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
          (transformer_blocks): ModuleList(
            (0-9): 10 x BasicTransformerBlock(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Attention(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): ModuleList(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Attention(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): ModuleList(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): FeedForward(
                (net): ModuleList(
                  (0): GEGLU(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
          )
          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
        )
      )
      (resnets): ModuleList(
        (0): ResnetBlock2D(
          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
          (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): ResnetBlock2D(
          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
        )
      )
    )
  )
  (up_blocks): ModuleList(
    (0): CrossAttnUpBlock2D(
      (attentions): ModuleList(
        (0-2): 3 x Transformer2DModel(
          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
          (transformer_blocks): ModuleList(
            (0-9): 10 x BasicTransformerBlock(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Attention(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): ModuleList(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Attention(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): ModuleList(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): FeedForward(
                (net): ModuleList(
                  (0): GEGLU(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
          )
          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
        )
      )
      (resnets): ModuleList(
        (0-1): 2 x ResnetBlock2D(
          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)
          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): ResnetBlock2D(
          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)
          (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (upsamplers): ModuleList(
        (0): Upsample2D(
          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (1): CrossAttnUpBlock2D(
      (attentions): ModuleList(
        (0-2): 3 x Transformer2DModel(
          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=640, out_features=640, bias=True)
          (transformer_blocks): ModuleList(
            (0-1): 2 x BasicTransformerBlock(
              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn1): Attention(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=640, out_features=640, bias=False)
                (to_v): Linear(in_features=640, out_features=640, bias=False)
                (to_out): ModuleList(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn2): Attention(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=2048, out_features=640, bias=False)
                (to_v): Linear(in_features=2048, out_features=640, bias=False)
                (to_out): ModuleList(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (ff): FeedForward(
                (net): ModuleList(
                  (0): GEGLU(
                    (proj): Linear(in_features=640, out_features=5120, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=2560, out_features=640, bias=True)
                )
              )
            )
          )
          (proj_out): Linear(in_features=640, out_features=640, bias=True)
        )
      )
      (resnets): ModuleList(
        (0): ResnetBlock2D(
          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)
          (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): ResnetBlock2D(
          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): ResnetBlock2D(
          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)
          (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (upsamplers): ModuleList(
        (0): Upsample2D(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (2): UpBlock2D(
      (resnets): ModuleList(
        (0): ResnetBlock2D(
          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)
          (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (1-2): 2 x ResnetBlock2D(
          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
          (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (mid_block): UNetMidBlock2DCrossAttn(
    (attentions): ModuleList(
      (0): Transformer2DModel(
        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
        (transformer_blocks): ModuleList(
          (0-9): 10 x BasicTransformerBlock(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Attention(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): ModuleList(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Attention(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): ModuleList(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): FeedForward(
              (net): ModuleList(
                (0): GEGLU(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
        )
        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
      )
    )
    (resnets): ModuleList(
      (0-1): 2 x ResnetBlock2D(
        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (nonlinearity): SiLU()
      )
    )
  )
  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)
  (conv_act): SiLU()
  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
"""